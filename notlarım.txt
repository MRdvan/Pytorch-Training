pytorch

torch.zeros_like(x) -> x ile aynı boyutta 0lar tensoru

torch.sin(a) -> a'yı değiştirmiyor
torch.sin_(b) -> b yi değiştiriyor
tensor.add_(5) -> in place

b=a yapınca pointer olarak atama gerçekleşiyor
b = a.clone() -> a nın kopyasını oluşturup atama yapılıyo
torch.eq(a, b) -> eşit mi?
-----------------------------------------------------------
if torch.cuda.is_available():
    print('We have a GPU!')   -> check gpu

if torch.cuda.is_available():
    my_device = torch.device('cuda') ->gpu device 
else:
    my_device = torch.device('cpu') -> cpu device

** x = torch.rand(2, 2, device=my_device) 
oluşturulan tensor gpu nun raminde oluşturuldu.

y = torch.rand(2, 2)   
y = y.to(my_device)  -> .to() ile cpu dan gpu rame taşınabilir 

kullandığın tüm variablellar aynı deviceda yer almıyorsa işlemleri yapamazsın.

------------------------------------------------------------
changing the number of dimentions
a = torch.rand(3, 226, 226) -> [1,3,226,226] olacak
b = a.unsqueeze(0) -> 0.ncı katmana 1 ekliyor.

1 eklemenin içerdeki boyutlara bir etkisi yok
c = torch.rand(1, 1, 1, 1, 1) ->  tensor([[[[[0.2347]]]]])

y = y.view([1,10]) -> ile reshape yapılabilir.

b = a.squeeze(0) -> 0.ncı katmanı yok ediyor. eğer o katmanın boyutu 1 ise.
-----------------------------------------------------------
numpy array to pytorch tensor 
numpy_array = np.ones((2, 3))
pytorch_tensor = torch.from_numpy(numpy_array)

pytorch tensor to numpy array
pytorch_rand = torch.rand(2, 3)
numpy_rand = pytorch_rand.numpy()

burda yapılan değişiklikler numpy halindeyken yapılsada torch değişkeninede etki ediyo 
------------------------------------------------------------------

 After calling optimizer.step(), you need to call optimizer.zero_grad() to refresh grads. or it will accumulate

a = torch.ones(2, 3, requires_grad=True)  -> autograd is on
a.requires_grad = False  ->  autograd is off,

with torch.no_grad():    -> bu da autograd off 
    c2 = a + b

x = a.detach() -> autograd historysinden ayrılmış bir kopya oluşturur.
---------------------------------------------------------------
dataset ve dataloader

training_data = datasets.FashionMNIST(    -> readytouse dataset
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

custom dataset için yeni bir class oluşturacağız
sonra dataloader ile batch batch diskten alacağız

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)

train_features, train_labels = next(iter(train_dataloader)) -> 64 lük batch aldık.

transform to modify the features and target_transform to modify the labels

------------------------------------------------------------
freeze all parameters of pretrained model

for param in model.parameters():
    param.requires_grad = False

-------------------------------------------------------------
Optimizasyon, her eğitim adımında model hatasını azaltmak için model parametrelerini ayarlama sürecidir.

optimazation steps:
1-call optim.zero_grad() to reset gradients
2-loss.backward() to backprop the prediction loss for each params
3- we have gradients, now we call optim.step(), adjust the params by the gradients
--------------------------------------------------------------

torch.nn.module eğitilmiş parametreleri alt sınıf olan torch.nn.parametre da tutar. bu torch.tensor ün bir alt sınıfıdır.
paremeters() methodu ile erişilebilirler.
modele inputu verdiğin zaman forward methodu otomatik olarak çağırılıyor.
nn.Sequential is an ordered container of modules

self.linear1 = torch.nn.Linear(100, 200)
(linear1): Linear(in_features=100, out_features=200, bias=True)
 self.activation = torch.nn.ReLU()

layer mxn ise weights mxn, bias nx1 boyutundadır. 
torch.nn.parameter default olarak autograd true olarak atanır. 

self.conv1 = torch.nn.Conv2d(1, 6, 5) 
1 -> input channels
6 -> how many kernel (output channels)
5 -> (5,5) kernel size

x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
--------------------------------------------------
saving model weights
model = models.vgg16(pretrained=True)
torch.save(model.state_dict(), 'model_weights.pth')

loading model weights(need to create that model first)
model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()
to save model structure as well
torch.save(model, 'model.pth')
to load model structure
model = torch.load('model.pth')

 